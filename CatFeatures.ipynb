{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RomulusGwelt/deep-learning-kbtu-2019/blob/master/CatFeatures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EqXuDEplOYwk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras, datetime\n",
        "from keras.layers import Input, Dense, Conv2D\n",
        "from keras.models import Model\n",
        "from keras.callbacks import TensorBoard, ModelCheckpoint, ReduceLROnPlateau\n",
        "# from keras.applications import mobilenetv2\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# !pip uninstall tensorflow\n",
        "# !pip install tensorflow-gpu\n",
        "\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth, files\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from keras_preprocessing.image import ImageDataGenerator\n",
        "\n",
        "import matplotlib.pyplot as plt \n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "\n",
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D, GlobalAveragePooling2D\n",
        "import keras\n",
        "from math import ceil\n",
        "from keras.applications.mobilenet_v2 import MobileNetV2\n",
        "from keras.applications.resnet50 import ResNet50\n",
        "from keras.applications.inception_v3 import InceptionV3\n",
        "# from keras.applications.inception_v3 import preprocess_input\n",
        "# from keras.applications.resnet50 import preprocess_input\n",
        "from keras.applications.xception import preprocess_input\n",
        "import os \n",
        "import tensorflow as tf\n",
        "from tensorflow.python.client import device_lib\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "# from cyclicLR import CyclicLR\n",
        "from keras.optimizers import Adam, SGD, RMSprop\n",
        "import datetime\n",
        "from keras.applications import xception\n",
        "import cv2\n",
        "# print(device_lib.list_local_devices())\n",
        "# sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\n",
        "# import tensorflow.compat.v1 as tf\n",
        "# tf.disable_v2_behavior()\n",
        "from google.colab.patches import cv2_imshow\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7DVjZ-nOfNn",
        "colab_type": "code",
        "outputId": "39315ccb-d4dc-41e7-8dc2-193fc0ca7293",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "img_size = 224\n",
        "\n",
        "mode = 'lmks'\n",
        "\n",
        "output_size = 18\n",
        "\n",
        "start_time = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
        "\n",
        "data_00 = np.load('lmks_CAT_00.npy', allow_pickle=True)\n",
        "data_01 = np.load('lmks_CAT_01.npy', allow_pickle=True)\n",
        "data_02 = np.load('lmks_CAT_02.npy', allow_pickle=True)\n",
        "data_03 = np.load('lmks_CAT_03.npy', allow_pickle=True)\n",
        "data_04 = np.load('lmks_CAT_04.npy', allow_pickle=True)\n",
        "data_05 = np.load('lmks_CAT_05.npy', allow_pickle=True)\n",
        "data_06 = np.load('lmks_CAT_06.npy', allow_pickle=True)\n",
        "\n",
        "print(\"Loaded data\")\n",
        "\n",
        "x_train = np.concatenate((data_00.item().get('imgs'), data_01.item().get('imgs'), data_02.item().get('imgs'), data_03.item().get('imgs'), data_04.item().get('imgs'), data_05.item().get('imgs')), axis=0)\n",
        "y_train = np.concatenate((data_00.item().get(mode), data_01.item().get(mode), data_02.item().get(mode), data_03.item().get(mode), data_04.item().get(mode), data_05.item().get(mode)), axis=0)\n",
        "\n",
        "print(\"Merged data\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loaded data\n",
            "Merged data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2YjdxyhOhvq",
        "colab_type": "code",
        "outputId": "d2c8071d-5fb8-4e93-80db-245ef4868106",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x_test = np.array(data_06.item().get('imgs'))\n",
        "y_test = np.array(data_06.item().get(mode))\n",
        "\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "x_train = np.reshape(x_train, (-1, img_size, img_size, 3))\n",
        "x_test = np.reshape(x_test, (-1, img_size, img_size, 3))\n",
        "\n",
        "y_train = np.reshape(y_train, (-1, output_size))\n",
        "y_test = np.reshape(y_test, (-1, output_size))\n",
        "\n",
        "print(\"Here goes the NN\")\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Here goes the NN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3v39CMdOkHI",
        "colab_type": "code",
        "outputId": "836722c0-6142-4a35-ea84-007b37366eaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "input_shape = (224, 224, 3)\n",
        "base_model = xception.Xception(input_shape=input_shape, weights='imagenet', include_top=False)\n",
        "\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dropout(0.5)(x)\n",
        "x = Dense(1024, activation='relu')(x)\n",
        "x = Dropout(0.5)(x)\n",
        "predictions = Dense(18)(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.4/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 3s 0us/step\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWuI4c15OlB4",
        "colab_type": "code",
        "outputId": "5d0bd4ee-3f02-4381-b729-6d0849c7e663",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "start_time = datetime.datetime.now().strftime('%Y_%m_%d_%H_%M_%S')\n",
        "print(\"ALMOST THERE\")\n",
        "\n",
        "# model.summary()\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5)\n",
        "# training\n",
        "model.compile(optimizer=keras.optimizers.Adam(), loss='mse')\n",
        "\n",
        "\n",
        "model.fit(x_train, y_train, epochs=50, batch_size=32, shuffle=True,\n",
        "  validation_data=(x_test, y_test), verbose=1,\n",
        "  callbacks=[\n",
        "    # TensorBoard(log_dir='logs/%s' % (start_time)),\n",
        "    ModelCheckpoint('./models/%s.h5' % (start_time), monitor='val_loss', verbose=1, save_best_only=True, mode='auto'),\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, verbose=1, mode='auto')\n",
        "  ]\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ALMOST THERE\n",
            "Train on 8609 samples, validate on 1388 samples\n",
            "Epoch 1/50\n",
            "8609/8609 [==============================] - 201s 23ms/step - loss: 111.4208 - val_loss: 75.5641\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 75.56409, saving model to ./models/2019_12_02_22_33_39.h5\n",
            "Epoch 2/50\n",
            "8609/8609 [==============================] - 197s 23ms/step - loss: 110.9420 - val_loss: 65.0149\n",
            "\n",
            "Epoch 00002: val_loss improved from 75.56409 to 65.01486, saving model to ./models/2019_12_02_22_33_39.h5\n",
            "Epoch 3/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 112.5181 - val_loss: 77.6304\n",
            "\n",
            "Epoch 00003: val_loss did not improve from 65.01486\n",
            "Epoch 4/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 110.2305 - val_loss: 77.4503\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 65.01486\n",
            "Epoch 5/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 108.7348 - val_loss: 57.4514\n",
            "\n",
            "Epoch 00005: val_loss improved from 65.01486 to 57.45136, saving model to ./models/2019_12_02_22_33_39.h5\n",
            "Epoch 6/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 107.2851 - val_loss: 62.3358\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 57.45136\n",
            "Epoch 7/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 108.9199 - val_loss: 103.0368\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 57.45136\n",
            "Epoch 8/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 109.1946 - val_loss: 68.9167\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 57.45136\n",
            "Epoch 9/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 105.9242 - val_loss: 61.6633\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 57.45136\n",
            "Epoch 10/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 103.9138 - val_loss: 70.0473\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 57.45136\n",
            "\n",
            "Epoch 00010: ReduceLROnPlateau reducing learning rate to 0.00020000000949949026.\n",
            "Epoch 11/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 99.5336 - val_loss: 49.0043\n",
            "\n",
            "Epoch 00011: val_loss improved from 57.45136 to 49.00432, saving model to ./models/2019_12_02_22_33_39.h5\n",
            "Epoch 12/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 93.7604 - val_loss: 52.5102\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 49.00432\n",
            "Epoch 13/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 94.3321 - val_loss: 42.7496\n",
            "\n",
            "Epoch 00013: val_loss improved from 49.00432 to 42.74963, saving model to ./models/2019_12_02_22_33_39.h5\n",
            "Epoch 14/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 94.9117 - val_loss: 46.0874\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 42.74963\n",
            "Epoch 15/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 92.9314 - val_loss: 45.7793\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 42.74963\n",
            "Epoch 16/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 92.0263 - val_loss: 44.9976\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 42.74963\n",
            "Epoch 17/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 92.4881 - val_loss: 46.3275\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 42.74963\n",
            "Epoch 18/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 89.9923 - val_loss: 46.4968\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 42.74963\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 4.0000001899898055e-05.\n",
            "Epoch 19/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 91.8472 - val_loss: 47.2547\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 42.74963\n",
            "Epoch 20/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 89.1735 - val_loss: 46.3828\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 42.74963\n",
            "Epoch 21/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 88.5713 - val_loss: 45.3609\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 42.74963\n",
            "Epoch 22/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 88.3032 - val_loss: 45.4728\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 42.74963\n",
            "Epoch 23/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 88.8513 - val_loss: 46.2966\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 42.74963\n",
            "\n",
            "Epoch 00023: ReduceLROnPlateau reducing learning rate to 8.000000525498762e-06.\n",
            "Epoch 24/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 88.7150 - val_loss: 45.3571\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 42.74963\n",
            "Epoch 25/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 89.7861 - val_loss: 44.7933\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 42.74963\n",
            "Epoch 26/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 88.5532 - val_loss: 45.1819\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 42.74963\n",
            "Epoch 27/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 87.7414 - val_loss: 44.7731\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 42.74963\n",
            "Epoch 28/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 88.5651 - val_loss: 45.3206\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 42.74963\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 1.6000001778593287e-06.\n",
            "Epoch 29/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 88.2492 - val_loss: 44.8940\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 42.74963\n",
            "Epoch 30/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 87.2028 - val_loss: 44.9349\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 42.74963\n",
            "Epoch 31/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 88.3125 - val_loss: 45.0689\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 42.74963\n",
            "Epoch 32/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 87.9015 - val_loss: 44.9035\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 42.74963\n",
            "Epoch 33/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 87.1842 - val_loss: 45.2340\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 42.74963\n",
            "\n",
            "Epoch 00033: ReduceLROnPlateau reducing learning rate to 3.200000264769187e-07.\n",
            "Epoch 34/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 87.3242 - val_loss: 44.9141\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 42.74963\n",
            "Epoch 35/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 89.9998 - val_loss: 44.9790\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 42.74963\n",
            "Epoch 36/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 88.8442 - val_loss: 44.9060\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 42.74963\n",
            "Epoch 37/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 89.1431 - val_loss: 44.8352\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 42.74963\n",
            "Epoch 38/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 88.4847 - val_loss: 45.0071\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 42.74963\n",
            "\n",
            "Epoch 00038: ReduceLROnPlateau reducing learning rate to 6.400000529538374e-08.\n",
            "Epoch 39/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 89.2791 - val_loss: 44.9194\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 42.74963\n",
            "Epoch 40/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 87.6570 - val_loss: 44.8134\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 42.74963\n",
            "Epoch 41/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 87.7443 - val_loss: 44.7671\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 42.74963\n",
            "Epoch 42/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 87.6760 - val_loss: 44.8770\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 42.74963\n",
            "Epoch 43/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 88.0601 - val_loss: 44.9949\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 42.74963\n",
            "\n",
            "Epoch 00043: ReduceLROnPlateau reducing learning rate to 1.2800001059076749e-08.\n",
            "Epoch 44/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 87.4372 - val_loss: 44.7752\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 42.74963\n",
            "Epoch 45/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 88.4262 - val_loss: 44.8293\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 42.74963\n",
            "Epoch 46/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 89.2499 - val_loss: 44.8956\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 42.74963\n",
            "Epoch 47/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 87.7831 - val_loss: 44.9491\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 42.74963\n",
            "Epoch 48/50\n",
            "8609/8609 [==============================] - 199s 23ms/step - loss: 87.5896 - val_loss: 44.6692\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 42.74963\n",
            "\n",
            "Epoch 00048: ReduceLROnPlateau reducing learning rate to 2.5600002118153498e-09.\n",
            "Epoch 49/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 88.9545 - val_loss: 44.7596\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 42.74963\n",
            "Epoch 50/50\n",
            "8609/8609 [==============================] - 198s 23ms/step - loss: 87.7382 - val_loss: 44.7959\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 42.74963\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1996add128>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JnmaXUNZfns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('model_cats.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06ZSnbYYZleb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4m2p1GyZmPV",
        "colab_type": "code",
        "outputId": "5aeb90b4-e626-4a44-8e49-64a9f14c80dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "uploaded = drive.CreateFile({'title': 'model_cats.h5'})\n",
        "uploaded.SetContentFile('model_cats.h5')\n",
        "uploaded.Upload()\n",
        "# uploaded.Download()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1m9qT8ySfH77GyIzEzurVPFcE1JMkNpI-\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}